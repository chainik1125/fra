Feature-Resolved Attention Analysis Summary
==================================================

Configuration:
  Model: GPT-2 Small
  Layer: 5
  Head: 0
  Samples analyzed: 20
  Processing time: 67.0s (3.3s per sample)

Results:
  Total unique feature pairs: 4,214,465
  Top pairs extracted: 200

  Self-interactions in top pairs: 72

Top 20 Feature Interactions:
--------------------------------------------------
  1. F25247 → F25247 (self) avg=3.9578 count=1
  2. F44620 → F44620 (self) avg=3.4443 count=1
  3. F23477 → F23477 (self) avg=2.8624 count=1
  4. F47820 → F47820 (self) avg=2.7544 count=1
  5. F31300 → F31300 (self) avg=2.7168 count=1
  6. F21799 → F21799 (self) avg=2.7120 count=3
  7. F 9937 → F 9937 (self) avg=2.4498 count=3
  8. F 1535 → F 1535 (self) avg=2.4079 count=1
  9. F31300 → F23477        avg=2.3760 count=1
 10. F44620 → F35305        avg=2.2427 count=1
 11. F 1507 → F31300        avg=2.1190 count=2
 12. F18687 → F 3598        avg=2.0507 count=1
 13. F18687 → F18687 (self) avg=2.0296 count=1
 14. F 1507 → F23477        avg=2.0189 count=2
 15. F44620 → F 3598        avg=1.9472 count=1
 16. F 1507 → F 1507 (self) avg=1.9110 count=3
 17. F 1507 → F44620        avg=1.9029 count=2
 18. F45555 → F27178        avg=1.8728 count=2
 19. F 1507 → F 3598        avg=1.8203 count=2
 20. F20778 → F20778 (self) avg=1.7775 count=6

==================================================
Analysis complete!
